providers:
  openrouter:
    driver: "openai_responses"
    base_url: "https://openrouter.ai/api/v1/responses"
    api_key_env: "OPENROUTER_API_KEY"
    models:
      qwen/qwen3-coder:free:
        supported_parameters: ["tool", "tool_choice", "max_tokens", "temperature", "top_p", "stop", "frequency_penalty", "presence_penalty", "seed", "top_k", "repetition_penalty"]
      qwen/qwen3-235b-a22b:free:
        supported_parameters: ["tool", "tool_choice", "max_tokens", "temperature", "top_p", "stop", "frequency_penalty", "presence_penalty", "seed", "top_k", "response_format"]
      moonshotai/kimi-dev-72b:free:
        supported_parameters: ["max_tokens", "temperature", "top_p", "stop", "frequency_penalty", "presence_penalty", "seed", "top_k", "repetition_penalty"]
  google_gemini:
    driver: "openai_chat"
    base_url: "https://generativelanguage.googleapis.com/v1beta/openai/"
    api_key_env: "GEMINI_API_KEY"
    models:
      gemini-2.5-flash:
        supported_parameters: ["reasoning_effort", "stream", "tools", "tool_choice", "response_format", "extra_body.thinking_config"]
  github_models:
    driver: "openai_chat"
    base_url: "https://models.github.ai/inference"
    api_key_env: "GITHUB_TOKEN"
    models:
      mistral-ai/Codestral-2501:
        supported_parameters: ["tool", "tool_choice", "max_tokens", "temperature", "top_p", "stop", "frequency_penalty", "presence_penalty", "seed", "top_k", "response_format"]
      microsoft/Phi-4:
        supported_parameters: ["tool", "tool_choice", "max_tokens", "temperature", "top_p", "stop", "frequency_penalty", "presence_penalty", "seed", "top_k", "response_format"]
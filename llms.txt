# Local Coding Assistant (LOCCA)

A sophisticated local-first Python project for building an advanced AI coding assistant with agent loops, tool systems, session management, LangGraph compatibility, and sophisticated opar-agent implementation.

---

## Goals
- **Local-first assistant** with layered configuration and provider routing.
- **Observe-plan-act-reflect architecture** across legacy loop and LangGraph agent.
- **Provider module** with automatic discovery, health checks, fallback policies.
- **Extensible tool + runtime layers** supporting session persistence.
- **Streaming LLM support** plus full CLI coverage.
- **High test coverage** spanning unit, integration, and E2E scenarios.

---

## Current Architecture Map

### Package: `src/local_coding_assistant/`

- **`agent/`**
  - `agent_loop.py`: Observe-plan-act-reflect orchestration, tool integration, session persistence.
  - `langgraph_agent.py`: LangGraph-compatible agent, graph node interface, streaming deltas.
  - `llm_manager.py`: Provider-aware request routing, streaming, retry + fallback orchestration.

- **`sandbox/`**
  - `manager.py`: Manages sandbox lifecycle, configuration, and execution requests.
  - `base.py`: Defines the ISandbox interface for sandbox implementations.
  - `sandbox_types.py`: Data models for sandbox execution requests/responses and metrics.
  - `guest/`: Sandbox guest environment components.
    - `session.py`: Handles code execution within the sandbox.
    - `resource_tracker.py`: Tracks resource usage and execution metrics.

- **`tools/`**
  - `builtin_tools/ptc_tools.py`: Implements Programmatic Tool Calling (PTC) with sandbox integration.
  - `sandbox_tools/`: Special tools available only in sandbox environment.
    - `compute_tools.py`: Math operations and computations.
    - `file_tools.py`: File system operations within sandbox.
    - `final_answer.py`: Tool for providing final answers in PTC flows.
  - `statistics.py`: Tracks and reports tool execution metrics and resource usage.
  - `tool_manager.py`: Manages tool registration and execution with metrics collection.

- **`providers/`**
  - `provider_manager.py`: Layered config merge (builtins, global YAML, local YAML), registration decorators, instantiation.
  - `router.py`: Policy-driven provider/model selection, health tracking, fallback logic.
  - `base.py`: Provider interfaces, request/response schemas, retry wrappers.
  - `generic_provider.py` + `compatible_drivers.py`: Adapters for OpenAI, OpenRouter, Google Gemini, local drivers.

- **`config/`**
  - `config_manager.py`: Three-layer hierarchy (global/session/call) with deep merge + validation.
  - `providers.default.yaml` / `providers.local.yaml`: Declarative provider definitions; CLI writes to local file.
  - `schemas.py`: Pydantic models (`AppConfig`, `ProviderConfig`, `LLMConfig`, `AgentConfig`).
  - `path_manager.py`: Environment-aware path resolution utility.

- **`runtime/`**
  - `runtime_manager.py`: Session orchestration, per-call overrides, streaming response assembly.

- **`core/`**
  - `bootstrap.py`: Loads `.env`, merges config, builds `LLMManager`, `ToolManager`, `RuntimeManager`.
  - `app_context.py`: Dependency container for CLI and integrations.

- **`cli/`**
  - `commands/provider.py`: `add|list|remove|validate|reload` provider management.
  - `commands/run.py`: `locca run query` entry point, supports `--provider`, `--model`, `--streaming`.
  - `commands/tool.py`: Tool management (list, add, remove, run, validate, reload)
  - `commands/config.py`: Config inspection and mutation via Typer.

- **`tests/`**
  - `integration/`: Validates bootstrap, provider routing, env handling, CLI flows.
  - `unit/`: Component-level coverage for agents, runtime, config utilities.
  - `e2e/`: CLI smoke tests via Typer runner.

#### `config/`
- **`config_manager.py`**: Multi-source configuration management:
  - Environment variable loading with LOCCA_ prefix
  - YAML configuration file support
  - Configuration validation and defaults
  - Runtime configuration updates

#### `cli/`
- **`main.py`**: Enhanced Typer application with comprehensive commands
- **`commands/`**:
  - `run.py`: Advanced query execution with agent loop
  - `tool.py`: Tool management (list, add, remove, run, validate, reload)
  - `config.py`: Configuration management interface
  - `serve.py`: Development server with hot reload

---

## Advanced Agent Patterns

### Observe-Plan-Act-Reflect Loop
LOCCA implements a sophisticated multistep reasoning pattern:

1. **Observe**: Gather information and context
2. **Plan**: Develop strategy and identify required tools
3. **Act**: Execute tools and process results
4. **Reflect**: Evaluate outcomes and plan next steps

Each phase includes error handling, retry logic, and proper state management.

### Session Management
- Persistent sessions with configurable history limits
- Context awareness across multiple queries
- Session isolation and cleanup
- Integration with runtime configuration

### Tool Integration
- Dynamic tool loading and registration
- JSON schema validation for tool inputs/outputs
- Automatic tool discovery and calling
- Tool result caching and error handling

---

## LangGraph Opar-Agent

### Advanced Graph-Based Agent Implementation

The LangGraph opar-agent (`langgraph_agent.py`) provides sophisticated multistep reasoning capabilities:

#### Core Architecture
- **Graph-Based Execution**: Uses LangGraph for orchestrating complex reasoning workflows
- **Four-Phase Pattern**: Implements observe-plan-act-reflect cycle as LangGraph nodes
- **State Management**: Sophisticated state handling across multiple execution phases
- **Streaming Integration**: Real-time streaming support for all agent phases

#### Node Implementations
1. **Observe Node**: Context gathering and information synthesis
2. **Plan Node**: Strategic planning with LLM-powered reasoning and tool identification
3. **Act Node**: Tool execution with comprehensive result processing
4. **Reflect Node**: Outcome analysis and learning from results

#### Key Features
- **Error Recovery**: Robust error handling with fallback mechanisms in each node
- **State Persistence**: Maintains complex state across graph execution
- **Streaming Support**: Real-time updates during long-running operations
- **Tool Integration**: Seamless integration with the tool registry system
- **Session Awareness**: Context management across multiple interactions

#### Usage Patterns
```python
# Create and run LangGraph opar-agent
agent = LangGraphAgent(
    llm_manager=llm,
    tool_manager=tools,
    max_iterations=5,
    streaming=True
)
result = await agent.run()
```

---

## LangGraph Integration

### Graph Compatibility
- **AgentLoop as Execution Engine**: Can serve as nodes in LangGraph workflows
- **State Management**: Proper state passing between graph nodes
- **Node Interface**: Implements LangGraph-compatible node interface
- **Orchestration**: Supports complex graph-based orchestration patterns

### Mock Implementation
The test suite includes `MockLangGraph` and `MockLangGraphNode` for:
- Testing graph orchestration patterns
- Validating node isolation and state management
- Demonstrating integration capabilities

---

## CLI Interface

### Key commands
```bash
# LLM queries
uv run locca run query "Implement DFS" --provider openrouter --model "openrouter:qwen/qwen3-coder:free" --streaming

# Provider lifecycle
uv run locca provider add openrouter --base-url https://openrouter.ai/api/v1 --api-key-env OPENROUTER_API_KEY --models qwen/qwen3-coder:free --dev
uv run locca provider list
uv run locca provider validate --dev
uv run locca provider remove openrouter --dev

# Config inspection
uv run locca config show
uv run locca config set LLM__TEMPERATURE 0.6

# Sandbox commands
uv run locca sandbox run "print('Hello, Sandbox!')"  # Run Python code
uv run locca sandbox exec "ls -la"  # Execute shell command
uv run locca sandbox stop <session_id>  # Stop a sandbox session

# Tool management
uv run locca tool list  # List all available tools
uv run locca tool info <tool_name>  # Show tool details

# Provider management
uv run locca provider list
uv run locca provider add --name my-provider --type openai --api-key $API_KEY

# Run a query with a specific provider
uv run locca run --provider my-provider "Your query here"
```

### Development features
- `bootstrap()` builds context with logging level auto-derived from config.
- CLI commands guard entrypoints with `safe_entrypoint` for consistent error handling.
- Provider commands autogenerate YAML skeletons and reload via `LLMManager.reload_providers()`.

---

## Testing Architecture

### Test Categories
- **`tests/unit/`**: Fast component tests with mocks
- **`tests/integration/`**: Integration tests with comprehensive fixtures
- **`tests/e2e/`**: End-to-end CLI testing

### Key Testing Features
- **Centralized Fixtures**: `conftest.py` with LLM, tool, and runtime mocks
- **LangGraph Compatibility Tests**: Graph orchestration validation
- **Streaming Tests**: Real-time response handling
- **Error Recovery Tests**: Failure mode validation

### Test Infrastructure
- **MockLLMManager**: Configurable LLM responses for testing
- **MockToolManager**: Tool registry with predefined behaviors
- **MockRuntimeManager**: Session and context management testing

---

## Configuration System

### Multi-Source Configuration
1. **Environment Variables**: `LOCCA_*` prefixed variables
2. **YAML Files**: Structured configuration files
3. **Runtime Defaults**: Sensible fallback values

### Key Configuration Areas
- **LLM Settings**: Model, temperature, tokens, provider
- **Runtime Settings**: Sessions, logging, error handling
- **Tool Settings**: Registration paths, validation modes

---

## Recent Major Additions

### LangGraph Opar-Agent Implementation
- Sophisticated graph-based agent with observe-plan-act-reflect nodes
- Advanced state management across multiple execution phases
- Streaming support for all agent phases
- Robust error handling with fallback mechanisms

### Agent Loop Implementation
- Full observe-plan-act-reflect pattern
- Tool integration with result processing
- Session persistence and context management
- Error recovery and retry mechanisms

### LangGraph Compatibility
- Graph-based orchestration support
- Node interface implementation
- State management between nodes
- Integration testing framework

### Streaming LLM Support
- Real-time response streaming
- Tool call streaming
- Progress updates during execution
- Enhanced user experience

### Enhanced Tool System
- JSON schema validation
- Dynamic tool loading
- Improved error handling
- Tool discovery and management

### Session & Context Management
- Persistent session storage
- Context awareness across interactions
- Session lifecycle management
- Integration with runtime configuration

### CLI Enhancement
- Comprehensive command set
- Tool management commands
- Configuration interface
- Development server

### Error Handling & Logging
- Centralized error handling
- Structured logging throughout
- Safe entry point decorators
- Detailed error reporting

---

## Extension Points

### Tool Development
- **Registration**: Add tool configuration to `config/tools.local.yaml`
  ```yaml
  tools:
    - name: my_tool
      module: my_tools.package.module  # Python import path
      # OR
      path: path/to/tool_implementation.py  # Path to Python file
      enabled: true
      config:  # Optional tool-specific configuration
        param1: value1
  ```

- **Implementation Requirements**:
  - Implement a class with a `run` method (optionally async)
  - Define input/output schemas using Pydantic models
  - Support streaming by yielding results (optional)
  - Handle errors with appropriate exceptions

- **Example Tool**:
  ```python
  from pydantic import BaseModel
  from typing import Any, AsyncIterator, Optional

  class ToolInput(BaseModel):
      query: str
      max_results: int = 5

  class ToolOutput(BaseModel):
      results: list[dict[str, Any]]
      count: int

  class MyTool:
      name = "my_tool"
      description = "My custom tool description"
      
      async def run(self, input_data: dict) -> dict:
          # Process input and return results
          return {"results": [...], "count": 0}
          
      # Optional: For streaming support
      async def stream(self, input_data: dict) -> AsyncIterator[dict]:
          yield {"partial": "result"}
  ```

- **Validation**:
  - Tools are validated on registration
  - Input/output schemas are automatically generated from type hints
  - Configuration is type-checked against tool's expected schema

### LLM Provider Integration
- Implement `LLMManager` interface
- Add provider-specific configuration
- Support streaming and tool calling
- Handle provider-specific errors

### Graph Node Development
- Implement LangGraph node interface
- Handle state passing between nodes
- Support different execution patterns
- Maintain node isolation

### Configuration Extension
- Add new configuration sources
- Implement custom validation
- Support runtime configuration updates
- Maintain backward compatibility

---

## Conventions

- **Async First**: All I/O operations are properly async
- **Error Handling**: Comprehensive error handling with custom exceptions
- **Logging**: Structured logging with appropriate levels
- **Testing**: High test coverage with integration and e2e tests
- **Configuration**: Multi-source with clear precedence rules
- **Dependencies**: Minimal external dependencies with clear justifications

---

## Quick Links
- `src/local_coding_assistant/providers/provider_manager.py`
- `src/local_coding_assistant/providers/router.py`
- `src/local_coding_assistant/agent/llm_manager.py`
- `src/local_coding_assistant/config/config_manager.py`
- `src/local_coding_assistant/core/bootstrap.py`
- `tests/integration/test_provider_manager_integration.py`
- `tests/integration/test_config_integration.py`
- `tests/unit/runtime/test_runtime_manager.py`

This document reflects the current sophisticated state of LOCCA with advanced AI patterns, robust architecture, comprehensive testing, and the new LangGraph opar-agent implementation. Update after any major architectural changes.